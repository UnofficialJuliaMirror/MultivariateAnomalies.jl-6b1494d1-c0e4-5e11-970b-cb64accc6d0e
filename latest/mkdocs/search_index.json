{
    "docs": [
        {
            "location": "/", 
            "text": "MultivariateAnomalies.jl\n\n\nA julia package for detecting multivariate anomalies.\n\n\nKeywords: Novelty detection, Anomaly Detection, Outlier Detection, Statistical Process Control\n\n\nPlease cite this package as ...\n\n\n\n\n\n\nRequirements\n\n\n\n\nJulia \n0.4\n\n\nDistances\n, \nMultivariateStats\n\n\nlatest \nLIBSVM\n branch via: \nPkg.clone(\"https://github.com/milanflach/LIBSVM.jl.git\");\n \nPkg.checkout(\"LIBSVM\", \"mutating_versions\");\n \nPkg.build(\"LIBSVM\")\n\n\n\n\n\n\nPackage Features\n\n\n\n\n\n\nDetect anomalies in your data with easy to use \nhigh level functions\n or individual \nanomaly detection algorithms\n\n\n\n\n\n\nFeature Extraction\n: Preprocess your data by extracting relevant features\n\n\n\n\n\n\nSimilarities and Dissimilarities\n: Compute distance matrices, kernel matrices and k-nearest neighbor objects.\n\n\n\n\n\n\nPostprocessing\n: Postprocess your anomaly scores, by computing their quantiles or combinations of several algorithms (ensembles).\n\n\n\n\n\n\nAUC\n: Compute the area under the curve as external evaluation metric of your scores.\n\n\n\n\n\n\n\n\nUsing the Package\n\n\nFor a quick start it might be useful to start with the \nhigh level functions\n for detecting anomalies. They can be used in highly automized way. \n\n\n\n\nInput Data\n\n\nMultivariateAnomalies.jl\n assumes that observations/samples/time steps are stored along the first dimension of the data array (rows of a matrix) with the number of observations \nT = size(data, 1)\n. Variables/attributes are stored along the last dimension \nN\n of the data array (along the columns of a matrix) with the number of variables \nVAR = size(data, N)\n. The implemented anomaly detection algorithms return anomaly scores indicating which observation(s) of the data are anomalous.\n\n\n\n\nIndex\n\n\n\n\nMultivariateAnomalies.EWMA\n\n\nMultivariateAnomalies.EWMA!\n\n\nMultivariateAnomalies.TDE\n\n\nMultivariateAnomalies.get_MedianCycle\n\n\nMultivariateAnomalies.get_MedianCycle!\n\n\nMultivariateAnomalies.get_MedianCycles\n\n\nMultivariateAnomalies.globalICA\n\n\nMultivariateAnomalies.globalPCA\n\n\nMultivariateAnomalies.init_MedianCycle\n\n\nMultivariateAnomalies.mw_COR\n\n\nMultivariateAnomalies.mw_VAR\n\n\nMultivariateAnomalies.sMSC\n\n\nMultivariateAnomalies.KDE\n\n\nMultivariateAnomalies.KDE!\n\n\nMultivariateAnomalies.KNFST_predict\n\n\nMultivariateAnomalies.KNFST_predict!\n\n\nMultivariateAnomalies.KNFST_train\n\n\nMultivariateAnomalies.KNN_Delta\n\n\nMultivariateAnomalies.KNN_Delta!\n\n\nMultivariateAnomalies.KNN_Gamma\n\n\nMultivariateAnomalies.KNN_Gamma!\n\n\nMultivariateAnomalies.REC\n\n\nMultivariateAnomalies.REC!\n\n\nMultivariateAnomalies.SVDD_predict\n\n\nMultivariateAnomalies.SVDD_predict!\n\n\nMultivariateAnomalies.SVDD_train\n\n\nMultivariateAnomalies.T2\n\n\nMultivariateAnomalies.T2!\n\n\nMultivariateAnomalies.UNIV\n\n\nMultivariateAnomalies.UNIV!\n\n\nMultivariateAnomalies.init_KDE\n\n\nMultivariateAnomalies.init_KNFST\n\n\nMultivariateAnomalies.init_KNN_Delta\n\n\nMultivariateAnomalies.init_KNN_Gamma\n\n\nMultivariateAnomalies.init_REC\n\n\nMultivariateAnomalies.init_SVDD_predict\n\n\nMultivariateAnomalies.init_T2\n\n\nMultivariateAnomalies.init_UNIV\n\n\nMultivariateAnomalies.compute_ensemble\n\n\nMultivariateAnomalies.get_quantile_scores\n\n\nMultivariateAnomalies.get_quantile_scores!\n\n\nMultivariateAnomalies.auc\n\n\nMultivariateAnomalies.auc_fpr_tpr\n\n\nMultivariateAnomalies.boolevents\n\n\nMultivariateAnomalies.dist_matrix\n\n\nMultivariateAnomalies.dist_matrix!\n\n\nMultivariateAnomalies.init_dist_matrix\n\n\nMultivariateAnomalies.init_knn_dists\n\n\nMultivariateAnomalies.kernel_matrix\n\n\nMultivariateAnomalies.kernel_matrix!\n\n\nMultivariateAnomalies.knn_dists\n\n\nMultivariateAnomalies.knn_dists!", 
            "title": "Home"
        }, 
        {
            "location": "/#multivariateanomaliesjl", 
            "text": "A julia package for detecting multivariate anomalies.  Keywords: Novelty detection, Anomaly Detection, Outlier Detection, Statistical Process Control  Please cite this package as ...", 
            "title": "MultivariateAnomalies.jl"
        }, 
        {
            "location": "/#requirements", 
            "text": "Julia  0.4  Distances ,  MultivariateStats  latest  LIBSVM  branch via:  Pkg.clone(\"https://github.com/milanflach/LIBSVM.jl.git\");   Pkg.checkout(\"LIBSVM\", \"mutating_versions\");   Pkg.build(\"LIBSVM\")", 
            "title": "Requirements"
        }, 
        {
            "location": "/#package-features", 
            "text": "Detect anomalies in your data with easy to use  high level functions  or individual  anomaly detection algorithms    Feature Extraction : Preprocess your data by extracting relevant features    Similarities and Dissimilarities : Compute distance matrices, kernel matrices and k-nearest neighbor objects.    Postprocessing : Postprocess your anomaly scores, by computing their quantiles or combinations of several algorithms (ensembles).    AUC : Compute the area under the curve as external evaluation metric of your scores.", 
            "title": "Package Features"
        }, 
        {
            "location": "/#using-the-package", 
            "text": "For a quick start it might be useful to start with the  high level functions  for detecting anomalies. They can be used in highly automized way.", 
            "title": "Using the Package"
        }, 
        {
            "location": "/#input-data", 
            "text": "MultivariateAnomalies.jl  assumes that observations/samples/time steps are stored along the first dimension of the data array (rows of a matrix) with the number of observations  T = size(data, 1) . Variables/attributes are stored along the last dimension  N  of the data array (along the columns of a matrix) with the number of variables  VAR = size(data, N) . The implemented anomaly detection algorithms return anomaly scores indicating which observation(s) of the data are anomalous.", 
            "title": "Input Data"
        }, 
        {
            "location": "/#index", 
            "text": "MultivariateAnomalies.EWMA  MultivariateAnomalies.EWMA!  MultivariateAnomalies.TDE  MultivariateAnomalies.get_MedianCycle  MultivariateAnomalies.get_MedianCycle!  MultivariateAnomalies.get_MedianCycles  MultivariateAnomalies.globalICA  MultivariateAnomalies.globalPCA  MultivariateAnomalies.init_MedianCycle  MultivariateAnomalies.mw_COR  MultivariateAnomalies.mw_VAR  MultivariateAnomalies.sMSC  MultivariateAnomalies.KDE  MultivariateAnomalies.KDE!  MultivariateAnomalies.KNFST_predict  MultivariateAnomalies.KNFST_predict!  MultivariateAnomalies.KNFST_train  MultivariateAnomalies.KNN_Delta  MultivariateAnomalies.KNN_Delta!  MultivariateAnomalies.KNN_Gamma  MultivariateAnomalies.KNN_Gamma!  MultivariateAnomalies.REC  MultivariateAnomalies.REC!  MultivariateAnomalies.SVDD_predict  MultivariateAnomalies.SVDD_predict!  MultivariateAnomalies.SVDD_train  MultivariateAnomalies.T2  MultivariateAnomalies.T2!  MultivariateAnomalies.UNIV  MultivariateAnomalies.UNIV!  MultivariateAnomalies.init_KDE  MultivariateAnomalies.init_KNFST  MultivariateAnomalies.init_KNN_Delta  MultivariateAnomalies.init_KNN_Gamma  MultivariateAnomalies.init_REC  MultivariateAnomalies.init_SVDD_predict  MultivariateAnomalies.init_T2  MultivariateAnomalies.init_UNIV  MultivariateAnomalies.compute_ensemble  MultivariateAnomalies.get_quantile_scores  MultivariateAnomalies.get_quantile_scores!  MultivariateAnomalies.auc  MultivariateAnomalies.auc_fpr_tpr  MultivariateAnomalies.boolevents  MultivariateAnomalies.dist_matrix  MultivariateAnomalies.dist_matrix!  MultivariateAnomalies.init_dist_matrix  MultivariateAnomalies.init_knn_dists  MultivariateAnomalies.kernel_matrix  MultivariateAnomalies.kernel_matrix!  MultivariateAnomalies.knn_dists  MultivariateAnomalies.knn_dists!", 
            "title": "Index"
        }, 
        {
            "location": "/man/HighLevelFunctions/", 
            "text": "High Level Anomaly Detection Algorithms\n\n\nWe provide high-level convenience functions for detecting the anomalies. Namely the pair of \n\n\nP = getParameters(algorithms, training_data)\n  and \ndetectAnomalies(testing_data, P)\n\n\nsets standard choices of the Parameters \nP\n and hands the parameters as well as the algorithms choice over to detect the anomalies. \n\n\nCurrently supported algorithms include Kernel Density Estimation (\nalgorithms = [\"KDE\"]\n), Recurrences (\n\"REC\"\n), k-Nearest Neighbors algorithms (\n\"KNN-Gamma\"\n, \n\"KNN-Delta\"\n), Hotelling's $T^2$ (\n\"T2\"\n), Support Vector Data Description (\n\"SVDD\"\n) and Kernel Null Foley Summon Transform (\n\"KNFST\"\n). With \ngetParameters()\n it is also possible to compute output scores of multiple algorithms at once (\nalgorihtms = [\"KDE\", \"T2\"]\n), quantiles of the output anomaly scores (\nquantiles = true\n) and ensembles of the selected algorithms (e.g. \nensemble_method = \"mean\"\n). \n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.getParameters\n \n \nFunction\n.\n\n\ngetParameters(algorithms::Array{ASCIIString,1} = [\nREC\n, \nKDE\n], training_data::AbstractArray{tp, 2} = [NaN NaN])\n\n\n\n\nreturn an object of type PARAMS, given the \nalgorithms\n and some \ntraining_data\n as a matrix.\n\n\nArguments\n\n\n\n\nalgorithms\n: Subset of \n[\"REC\", \"KDE\", \"KNN_Gamma\", \"KNN_Delta\", \"SVDD\", \"KNFST\", \"T2\"]\n\n\ntraining_data\n: data for training the algorithms / for getting the Parameters.\n\n\ndist::ASCIIString = \"Euclidean\"\n\n\nsigma_quantile::Float64 = 0.5\n (median): quantile of the distance matrix, used to compute the weighting parameter for the kernel matrix (\nalgorithms = [\"SVDD\", \"KNFST\", \"KDE\"]\n)\n\n\nvarepsilon_quantile\n = \nsigma_quantile\n by default: quantile of the distance matrix to compute the radius of the hyperball in which the number of reccurences is counted (\nalgorihtms = [\"REC\"]\n)\n\n\nk_perc::Float64 = 0.05\n: percentage of the first dimension of \ntraining_data\n to estimmate the number of nearest neighbors (\nalgorithms = [\"KNN-Gamma\", \"KNN_Delta\"]\n)\n\n\nnu::Float64 = 0.2\n: use the maximal percentage of outliers for \nalgorithms = [\"SVDD\"]\n\n\ntemp_excl::Int64 = 0\n. Exclude temporal adjacent points from beeing count as recurrences of k-nearest neighbors \nalgorithms = [\"REC\", \"KNN-Gamma\", \"KNN_Delta\"]\n\n\nensemble_method = \"None\"\n: compute an ensemble of the used algorithms. Possible choices (given in \ncompute_ensemble()\n) are \"mean\", \"median\", \"max\" and \"min\".\n\n\nquantiles = false\n: convert the output scores of the algorithms into quantiles.\n\n\n\n\nExamples\n\n\njulia\n training_data = randn(100, 2); testing_data = randn(100, 2);\njulia\n P = getParameters([\nREC\n, \nKDE\n, \nSVDD\n], training_data, quantiles = false);\njulia\n detectAnomalies(testing_data, P)\n\n\n\n\n#\n\n\nMultivariateAnomalies.detectAnomalies\n \n \nFunction\n.\n\n\ndetectAnomalies{tp, N}(data::AbstractArray{tp, N}, P::PARAMS)\ndetectAnomalies{tp, N}(data::AbstractArray{tp, N}, algorithms::Array{ASCIIString,1} = [\nREC\n, \nKDE\n]; mean = 0)\n\n\n\n\ndetect anomalies, given some Parameter object \nP\n of type PARAMS. Train the Parameters \nP\n with \ngetParameters()\n beforehand on some training data. See \ngetParameters()\n. Without training \nP\n beforehand, it is also possible to use \ndetectAnomalies(data, algorithms)\n given some algorithms (except SVDD, KNFST). Some default parameters are used in this case to initialize \nP\n internally.\n\n\nExamples\n\n\njulia\n training_data = randn(100, 2); testing_data = randn(100, 2);\njulia\n # compute the anoamly scores of the algorithms \nREC\n, \nKDE\n, \nT2\n and \nKNN_Gamma\n, their quantiles and return their ensemble scores\njulia\n P = getParameters([\nREC\n, \nKDE\n, \nT2\n, \nKNN_Gamma\n], training_data, quantiles = true, ensemble_method = \nmean\n);\njulia\n detectAnomalies(testing_data, P)\n\n\n\n\n#\n\n\nMultivariateAnomalies.detectAnomalies!\n \n \nFunction\n.\n\n\ndetectAnomalies!{tp, N}(data::AbstractArray{tp, N}, P::PARAMS)\n\n\n\n\nmutating version of \ndetectAnomalies()\n. Directly writes the output into \nP\n.\n\n\n#\n\n\nMultivariateAnomalies.init_detectAnomalies\n \n \nFunction\n.\n\n\ninit_detectAnomalies{tp, N}(data::AbstractArray{tp, N}, P::PARAMS)\n\n\n\n\ninitialize empty arrays in \nP\n for detecting the anomalies.\n\n\n\n\nIndex", 
            "title": "High Level Functions"
        }, 
        {
            "location": "/man/HighLevelFunctions/#high-level-anomaly-detection-algorithms", 
            "text": "We provide high-level convenience functions for detecting the anomalies. Namely the pair of   P = getParameters(algorithms, training_data)   and  detectAnomalies(testing_data, P)  sets standard choices of the Parameters  P  and hands the parameters as well as the algorithms choice over to detect the anomalies.   Currently supported algorithms include Kernel Density Estimation ( algorithms = [\"KDE\"] ), Recurrences ( \"REC\" ), k-Nearest Neighbors algorithms ( \"KNN-Gamma\" ,  \"KNN-Delta\" ), Hotelling's $T^2$ ( \"T2\" ), Support Vector Data Description ( \"SVDD\" ) and Kernel Null Foley Summon Transform ( \"KNFST\" ). With  getParameters()  it is also possible to compute output scores of multiple algorithms at once ( algorihtms = [\"KDE\", \"T2\"] ), quantiles of the output anomaly scores ( quantiles = true ) and ensembles of the selected algorithms (e.g.  ensemble_method = \"mean\" ).", 
            "title": "High Level Anomaly Detection Algorithms"
        }, 
        {
            "location": "/man/HighLevelFunctions/#functions", 
            "text": "#  MultivariateAnomalies.getParameters     Function .  getParameters(algorithms::Array{ASCIIString,1} = [ REC ,  KDE ], training_data::AbstractArray{tp, 2} = [NaN NaN])  return an object of type PARAMS, given the  algorithms  and some  training_data  as a matrix.  Arguments   algorithms : Subset of  [\"REC\", \"KDE\", \"KNN_Gamma\", \"KNN_Delta\", \"SVDD\", \"KNFST\", \"T2\"]  training_data : data for training the algorithms / for getting the Parameters.  dist::ASCIIString = \"Euclidean\"  sigma_quantile::Float64 = 0.5  (median): quantile of the distance matrix, used to compute the weighting parameter for the kernel matrix ( algorithms = [\"SVDD\", \"KNFST\", \"KDE\"] )  varepsilon_quantile  =  sigma_quantile  by default: quantile of the distance matrix to compute the radius of the hyperball in which the number of reccurences is counted ( algorihtms = [\"REC\"] )  k_perc::Float64 = 0.05 : percentage of the first dimension of  training_data  to estimmate the number of nearest neighbors ( algorithms = [\"KNN-Gamma\", \"KNN_Delta\"] )  nu::Float64 = 0.2 : use the maximal percentage of outliers for  algorithms = [\"SVDD\"]  temp_excl::Int64 = 0 . Exclude temporal adjacent points from beeing count as recurrences of k-nearest neighbors  algorithms = [\"REC\", \"KNN-Gamma\", \"KNN_Delta\"]  ensemble_method = \"None\" : compute an ensemble of the used algorithms. Possible choices (given in  compute_ensemble() ) are \"mean\", \"median\", \"max\" and \"min\".  quantiles = false : convert the output scores of the algorithms into quantiles.   Examples  julia  training_data = randn(100, 2); testing_data = randn(100, 2);\njulia  P = getParameters([ REC ,  KDE ,  SVDD ], training_data, quantiles = false);\njulia  detectAnomalies(testing_data, P)  #  MultivariateAnomalies.detectAnomalies     Function .  detectAnomalies{tp, N}(data::AbstractArray{tp, N}, P::PARAMS)\ndetectAnomalies{tp, N}(data::AbstractArray{tp, N}, algorithms::Array{ASCIIString,1} = [ REC ,  KDE ]; mean = 0)  detect anomalies, given some Parameter object  P  of type PARAMS. Train the Parameters  P  with  getParameters()  beforehand on some training data. See  getParameters() . Without training  P  beforehand, it is also possible to use  detectAnomalies(data, algorithms)  given some algorithms (except SVDD, KNFST). Some default parameters are used in this case to initialize  P  internally.  Examples  julia  training_data = randn(100, 2); testing_data = randn(100, 2);\njulia  # compute the anoamly scores of the algorithms  REC ,  KDE ,  T2  and  KNN_Gamma , their quantiles and return their ensemble scores\njulia  P = getParameters([ REC ,  KDE ,  T2 ,  KNN_Gamma ], training_data, quantiles = true, ensemble_method =  mean );\njulia  detectAnomalies(testing_data, P)  #  MultivariateAnomalies.detectAnomalies!     Function .  detectAnomalies!{tp, N}(data::AbstractArray{tp, N}, P::PARAMS)  mutating version of  detectAnomalies() . Directly writes the output into  P .  #  MultivariateAnomalies.init_detectAnomalies     Function .  init_detectAnomalies{tp, N}(data::AbstractArray{tp, N}, P::PARAMS)  initialize empty arrays in  P  for detecting the anomalies.", 
            "title": "Functions"
        }, 
        {
            "location": "/man/HighLevelFunctions/#index", 
            "text": "", 
            "title": "Index"
        }, 
        {
            "location": "/man/DetectionAlgorithms/", 
            "text": "Anomaly Detection Algorithms\n\n\nMost of the anomaly detection algorithms below work on a distance/similarity matrix \nD\n or a kernel/dissimilarity matrix \nK\n. They can be comuted using the functions provided \nhere\n.\n\n\nCurrently supported algorithms include\n\n\n\n\nRecurrences (REC)\n\n\nKernel Density Estimation (KDE)\n\n\nHotelling's $T^2$ (Mahalanobis distance) (T2)\n\n\ntwo k-Nearest Neighbor approaches (KNN-Gamma, KNN-Delta)  \n\n\nUnivariate Approach (UNIV)\n\n\nSupport Vector Data Description (SVDD)\n\n\nKernel Null Foley Summon Transform (KNFST)\n\n\n\n\n\n\nFunctions\n\n\n\n\nRecurrences\n\n\n#\n\n\nMultivariateAnomalies.REC\n \n \nFunction\n.\n\n\nREC(D::AbstractArray, rec_threshold::Float64, temp_excl::Int = 5)\n\n\n\n\nCount the number of observations (recurrences) which fall into a radius \nrec_threshold\n of a distance matrix \nD\n. Exclude steps which are closer than \ntemp_excl\n to be count as recurrences (default: \ntemp_excl = 5\n)\n\n\nMarwan, N., Carmen Romano, M., Thiel, M., \n Kurths, J. (2007). Recurrence plots for the analysis of complex systems. Physics Reports, 438(5-6), 237\u2013329. http://doi.org/10.1016/j.physrep.2006.11.001\n\n\n#\n\n\nMultivariateAnomalies.REC!\n \n \nFunction\n.\n\n\nREC!(rec_out::AbstractArray, D::AbstractArray, rec_threshold::Float64, temp_excl::Int = 5)\n\n\n\n\nMemory efficient version of \nREC()\n for use within a loop. \nrec_out\n is preallocated output, should be initialised with \ninit_REC()\n.\n\n\n#\n\n\nMultivariateAnomalies.init_REC\n \n \nFunction\n.\n\n\ninit_REC(D::Array{Float64, 2})\ninit_REC(T::Int)\n\n\n\n\nget object for memory efficient \nREC!()\n versions. Input can be a distance matrix \nD\n or the number of timesteps (observations) \nT\n.\n\n\n\n\nKernel Density Estimation\n\n\n#\n\n\nMultivariateAnomalies.KDE\n \n \nFunction\n.\n\n\nKDE(K)\n\n\n\n\nCompute a Kernel Density Estimation (the Parzen sum), given a Kernel matrix \nK\n.\n\n\nParzen, E. (1962). On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics, 33, 1\u20131065\u20131076.\n\n\n#\n\n\nMultivariateAnomalies.KDE!\n \n \nFunction\n.\n\n\nKDE!(KDE_out, K)\n\n\n\n\nMemory efficient version of \nKDE()\n. Additionally uses preallocated \nKDE_out\n object for writing the results. Initialize \nKDE_out\n with \ninit_KDE()\n.\n\n\n#\n\n\nMultivariateAnomalies.init_KDE\n \n \nFunction\n.\n\n\ninit_KDE(K::Array{Float64, 2})\ninit_KDE(T::Int)\n\n\n\n\nReturns \nKDE_out\n object for usage in \nKDE!()\n. Use either a Kernel matrix \nK\n or the number of time steps/observations \nT\n as argument.\n\n\n\n\nHotelling's $T^2$\n\n\n#\n\n\nMultivariateAnomalies.T2\n \n \nFunction\n.\n\n\nT2{tp}(data::AbstractArray{tp,2}, Q::AbstractArray[, mv])\n\n\n\n\nCompute Hotelling's T2 control chart (the squared Mahalanobis distance to the data's mean vector (\nmv\n), given the covariance matrix \nQ\n). Input data is a two dimensional data matrix (observations * variables).\n\n\nLowry, C. A., \n Woodall, W. H. (1992). A Multivariate Exponentially Weighted Moving Average Control Chart. Technometrics, 34, 46\u201353.\n\n\n#\n\n\nMultivariateAnomalies.T2!\n \n \nFunction\n.\n\n\nT2!(t2_out, data, Q[, mv])\n\n\n\n\nMemory efficient version of \nT2()\n, for usage within a loop etc. Initialize the \nt2_out\n object with \ninit_T2()\n. \nt2_out[1]\n contains the squred Mahalanobis distance after computation.\n\n\n#\n\n\nMultivariateAnomalies.init_T2\n \n \nFunction\n.\n\n\ninit_T2(VAR::Int, T::Int)\ninit_T2{tp}(data::AbstractArray{tp,2})\n\n\n\n\ninitialize \nt2_out\n object for \nT2!\n either with number of variables \nVAR\n and observations/time steps \nT\n or with a two dimensional \ndata\n matrix (time * variables)\n\n\n\n\nk-Nearest Neighbors\n\n\n#\n\n\nMultivariateAnomalies.KNN_Gamma\n \n \nFunction\n.\n\n\nKNN_Gamma(knn_dists_out)\n\n\n\n\nThis function computes the mean distance of the K nearest neighbors given a \nknn_dists_out\n object from \nknn_dists()\n as input argument.\n\n\nHarmeling, S., Dornhege, G., Tax, D., Meinecke, F., \n M\u00fcller, K.-R. (2006). From outliers to prototypes: Ordering data. Neurocomputing, 69(13-15), 1608\u20131618. http://doi.org/10.1016/j.neucom.2005.05.015\n\n\n#\n\n\nMultivariateAnomalies.KNN_Gamma!\n \n \nFunction\n.\n\n\nKNN_Gamma!(KNN_Gamma_out, knn_dists_out)\n\n\n\n\nMemory efficient version of \nKNN_Gamma\n, to be used in a loop. Initialize \nKNN_Gamma_out\n with \ninit_KNN_Gamma()\n.\n\n\n#\n\n\nMultivariateAnomalies.init_KNN_Gamma\n \n \nFunction\n.\n\n\ninit_KNN_Gamma(T::Int)\ninit_KNN_Gamma(knn_dists_out)\n\n\n\n\ninitialize a \nKNN_Gamma_out\n object for \nKNN_Gamma!\n either with \nT\n, the number of observations/time steps or with a \nknn_dists_out\n object.\n\n\n#\n\n\nMultivariateAnomalies.KNN_Delta\n \n \nFunction\n.\n\n\nKNN_Delta(knn_dists_out, data)\n\n\n\n\nCompute Delta as vector difference of the k-nearest neighbors. Arguments are a \nknn_dists()\n object (\nknn_dists_out\n) and a \ndata\n matrix (observations * variables)\n\n\nHarmeling, S., Dornhege, G., Tax, D., Meinecke, F., \n M\u00fcller, K.-R. (2006). From outliers to prototypes: Ordering data. Neurocomputing, 69(13-15), 1608\u20131618. http://doi.org/10.1016/j.neucom.2005.05.015\n\n\n#\n\n\nMultivariateAnomalies.KNN_Delta!\n \n \nFunction\n.\n\n\nKNN_Delta!(KNN_Delta_out, knn_dists_out, data)\n\n\n\n\nMemory Efficient Version of \nKNN_Delta()\n. \nKNN_Delta_out[1]\n is the vector difference of the k-nearest neighbors.\n\n\n#\n\n\nMultivariateAnomalies.init_KNN_Delta\n \n \nFunction\n.\n\n\ninit_KNN_Delta(T, VAR, k)\n\n\n\n\nreturn a \nKNN_Delta_out\n object to be used for \nKNN_Delta!\n. Input: time steps/observations \nT\n, variables \nVAR\n, number of K nearest neighbors \nk\n.\n\n\n\n\nUnivariate Approach\n\n\n#\n\n\nMultivariateAnomalies.UNIV\n \n \nFunction\n.\n\n\nUNIV(data)\n\n\n\n\norder the values in each varaible and return their maximum, i.e. any of the variables in \ndata\n (observations * variables) is above a given quantile, the highest quantile will be returned.\n\n\n#\n\n\nMultivariateAnomalies.UNIV!\n \n \nFunction\n.\n\n\nUNIV!(univ_out, data)\n\n\n\n\nMemory efficient version of \nUNIV()\n, input an \nuniv_out\n object from \ninit_UNIV()\n and some \ndata\n matrix observations * variables\n\n\n#\n\n\nMultivariateAnomalies.init_UNIV\n \n \nFunction\n.\n\n\ninit_UNIV(T::Int, VAR::Int)\ninit_UNIV{tp}(data::AbstractArray{tp, 2})\n\n\n\n\ninitialize a \nuniv_out\n object to be used in \nUNIV!()\n either with number of time steps/observations \nT\n and variables \nVAR\n or with a \ndata\n matrix observations * variables.\n\n\n\n\nSupport Vector Data Description\n\n\n#\n\n\nMultivariateAnomalies.SVDD_train\n \n \nFunction\n.\n\n\nSVDD_train(K, nu)\n\n\n\n\ntrain a one class support vecort machine model (i.e. support vector data description), given a kernel matrix K and and the highest possible percentage of outliers \nnu\n. Returns the model object (\nsvdd_model\n). Requires LIBSVM.\n\n\nTax, D. M. J., \n Duin, R. P. W. (1999). Support vector domain description. Pattern Recognition Letters, 20, 1191\u20131199. Sch\u00f6lkopf, B., Williamson, R. C., \n Bartlett, P. L. (2000). New Support Vector Algorithms. Neural Computation, 12, 1207\u20131245.\n\n\n#\n\n\nMultivariateAnomalies.SVDD_predict\n \n \nFunction\n.\n\n\nSVDD_predict(svdd_model, K)\n\n\n\n\npredict the outlierness of an object given the testing Kernel matrix \nK\n and the \nsvdd_model\n from SVDD_train(). Requires LIBSVM.\n\n\nTax, D. M. J., \n Duin, R. P. W. (1999). Support vector domain description. Pattern Recognition Letters, 20, 1191\u20131199. Sch\u00f6lkopf, B., Williamson, R. C., \n Bartlett, P. L. (2000). New Support Vector Algorithms. Neural Computation, 12, 1207\u20131245.\n\n\n#\n\n\nMultivariateAnomalies.SVDD_predict!\n \n \nFunction\n.\n\n\nSVDD_predict!(SVDD_out, svdd_model, K)\n\n\n\n\nMemory efficient version of \nSVDD_predict()\n. Additional input argument is the \nSVDD_out\n object from \ninit_SVDD_predict()\n. Compute \nK\nwith \nkernel_matrix()\n. \nSVDD_out[1]\n are predicted labels, \nSVDD_out[2]\n decision_values. Requires LIBSVM.\n\n\n#\n\n\nMultivariateAnomalies.init_SVDD_predict\n \n \nFunction\n.\n\n\ninit_SVDD_predict(T::Int)\ninit_SVDD_predict(T::Int, Ttrain::Int)\n\n\n\n\ninitializes a \nSVDD_out\n object to be used in \nSVDD_predict!()\n. Input is the number of time steps \nT\n (in prediction mode). If \nT\n for prediction differs from T of the training data (\nTtrain\n) use \nTtrain\n as additional argument.\n\n\n\n\nKernel Null Foley Summon Transform\n\n\n#\n\n\nMultivariateAnomalies.KNFST_train\n \n \nFunction\n.\n\n\nKNFST_train(K)\n\n\n\n\ntrain a one class novelty KNFST model on a Kernel matrix \nK\n according to Paul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.\n\n\nOutput\n\n\n(proj, targetValue)\n \nproj\n    \u2013 projection vector for data points (project x via kx*proj, where kx is row vector containing kernel values of x and training data) \ntargetValue\n \u2013 value of all training samples in the null space\n\n\n#\n\n\nMultivariateAnomalies.KNFST_predict\n \n \nFunction\n.\n\n\nKNFST_predict(model, K)\n\n\n\n\npredict the outlierness of some data (represented by the kernel matrix \nK\n), given some KNFST \nmodel\n from \nKNFST_train(K)\n. Compute \nK\nwith \nkernel_matrix()\n.\n\n\nPaul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.\n\n\n#\n\n\nMultivariateAnomalies.KNFST_predict!\n \n \nFunction\n.\n\n\nKNFST_predict!(KNFST_out, KNFST_mod, K)\n\n\n\n\npredict the outlierness of some data (represented by the kernel matrix \nK\n), given a \nKNFST_out\n object (\ninit_KNFST()\n), some KNFST model (\nKNFST_mod = KNFST_train(K)\n) and the testing kernel matrix K.\n\n\nPaul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.\n\n\n#\n\n\nMultivariateAnomalies.init_KNFST\n \n \nFunction\n.\n\n\ninit_KNFST(T, KNFST_mod)\n\n\n\n\ninitialize a \nKNFST_out\nobject for the use with \nKNFST_predict!\n, given \nT\n, the number of observations and the model output \nKNFST_train(K)\n.\n\n\n\n\nIndex", 
            "title": "Anomaly Detection Algorithms"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#anomaly-detection-algorithms", 
            "text": "Most of the anomaly detection algorithms below work on a distance/similarity matrix  D  or a kernel/dissimilarity matrix  K . They can be comuted using the functions provided  here .  Currently supported algorithms include   Recurrences (REC)  Kernel Density Estimation (KDE)  Hotelling's $T^2$ (Mahalanobis distance) (T2)  two k-Nearest Neighbor approaches (KNN-Gamma, KNN-Delta)    Univariate Approach (UNIV)  Support Vector Data Description (SVDD)  Kernel Null Foley Summon Transform (KNFST)", 
            "title": "Anomaly Detection Algorithms"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#functions", 
            "text": "", 
            "title": "Functions"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#recurrences", 
            "text": "#  MultivariateAnomalies.REC     Function .  REC(D::AbstractArray, rec_threshold::Float64, temp_excl::Int = 5)  Count the number of observations (recurrences) which fall into a radius  rec_threshold  of a distance matrix  D . Exclude steps which are closer than  temp_excl  to be count as recurrences (default:  temp_excl = 5 )  Marwan, N., Carmen Romano, M., Thiel, M.,   Kurths, J. (2007). Recurrence plots for the analysis of complex systems. Physics Reports, 438(5-6), 237\u2013329. http://doi.org/10.1016/j.physrep.2006.11.001  #  MultivariateAnomalies.REC!     Function .  REC!(rec_out::AbstractArray, D::AbstractArray, rec_threshold::Float64, temp_excl::Int = 5)  Memory efficient version of  REC()  for use within a loop.  rec_out  is preallocated output, should be initialised with  init_REC() .  #  MultivariateAnomalies.init_REC     Function .  init_REC(D::Array{Float64, 2})\ninit_REC(T::Int)  get object for memory efficient  REC!()  versions. Input can be a distance matrix  D  or the number of timesteps (observations)  T .", 
            "title": "Recurrences"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#kernel-density-estimation", 
            "text": "#  MultivariateAnomalies.KDE     Function .  KDE(K)  Compute a Kernel Density Estimation (the Parzen sum), given a Kernel matrix  K .  Parzen, E. (1962). On Estimation of a Probability Density Function and Mode. The Annals of Mathematical Statistics, 33, 1\u20131065\u20131076.  #  MultivariateAnomalies.KDE!     Function .  KDE!(KDE_out, K)  Memory efficient version of  KDE() . Additionally uses preallocated  KDE_out  object for writing the results. Initialize  KDE_out  with  init_KDE() .  #  MultivariateAnomalies.init_KDE     Function .  init_KDE(K::Array{Float64, 2})\ninit_KDE(T::Int)  Returns  KDE_out  object for usage in  KDE!() . Use either a Kernel matrix  K  or the number of time steps/observations  T  as argument.", 
            "title": "Kernel Density Estimation"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#hotellings-t2", 
            "text": "#  MultivariateAnomalies.T2     Function .  T2{tp}(data::AbstractArray{tp,2}, Q::AbstractArray[, mv])  Compute Hotelling's T2 control chart (the squared Mahalanobis distance to the data's mean vector ( mv ), given the covariance matrix  Q ). Input data is a two dimensional data matrix (observations * variables).  Lowry, C. A.,   Woodall, W. H. (1992). A Multivariate Exponentially Weighted Moving Average Control Chart. Technometrics, 34, 46\u201353.  #  MultivariateAnomalies.T2!     Function .  T2!(t2_out, data, Q[, mv])  Memory efficient version of  T2() , for usage within a loop etc. Initialize the  t2_out  object with  init_T2() .  t2_out[1]  contains the squred Mahalanobis distance after computation.  #  MultivariateAnomalies.init_T2     Function .  init_T2(VAR::Int, T::Int)\ninit_T2{tp}(data::AbstractArray{tp,2})  initialize  t2_out  object for  T2!  either with number of variables  VAR  and observations/time steps  T  or with a two dimensional  data  matrix (time * variables)", 
            "title": "Hotelling's $T^2$"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#k-nearest-neighbors", 
            "text": "#  MultivariateAnomalies.KNN_Gamma     Function .  KNN_Gamma(knn_dists_out)  This function computes the mean distance of the K nearest neighbors given a  knn_dists_out  object from  knn_dists()  as input argument.  Harmeling, S., Dornhege, G., Tax, D., Meinecke, F.,   M\u00fcller, K.-R. (2006). From outliers to prototypes: Ordering data. Neurocomputing, 69(13-15), 1608\u20131618. http://doi.org/10.1016/j.neucom.2005.05.015  #  MultivariateAnomalies.KNN_Gamma!     Function .  KNN_Gamma!(KNN_Gamma_out, knn_dists_out)  Memory efficient version of  KNN_Gamma , to be used in a loop. Initialize  KNN_Gamma_out  with  init_KNN_Gamma() .  #  MultivariateAnomalies.init_KNN_Gamma     Function .  init_KNN_Gamma(T::Int)\ninit_KNN_Gamma(knn_dists_out)  initialize a  KNN_Gamma_out  object for  KNN_Gamma!  either with  T , the number of observations/time steps or with a  knn_dists_out  object.  #  MultivariateAnomalies.KNN_Delta     Function .  KNN_Delta(knn_dists_out, data)  Compute Delta as vector difference of the k-nearest neighbors. Arguments are a  knn_dists()  object ( knn_dists_out ) and a  data  matrix (observations * variables)  Harmeling, S., Dornhege, G., Tax, D., Meinecke, F.,   M\u00fcller, K.-R. (2006). From outliers to prototypes: Ordering data. Neurocomputing, 69(13-15), 1608\u20131618. http://doi.org/10.1016/j.neucom.2005.05.015  #  MultivariateAnomalies.KNN_Delta!     Function .  KNN_Delta!(KNN_Delta_out, knn_dists_out, data)  Memory Efficient Version of  KNN_Delta() .  KNN_Delta_out[1]  is the vector difference of the k-nearest neighbors.  #  MultivariateAnomalies.init_KNN_Delta     Function .  init_KNN_Delta(T, VAR, k)  return a  KNN_Delta_out  object to be used for  KNN_Delta! . Input: time steps/observations  T , variables  VAR , number of K nearest neighbors  k .", 
            "title": "k-Nearest Neighbors"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#univariate-approach", 
            "text": "#  MultivariateAnomalies.UNIV     Function .  UNIV(data)  order the values in each varaible and return their maximum, i.e. any of the variables in  data  (observations * variables) is above a given quantile, the highest quantile will be returned.  #  MultivariateAnomalies.UNIV!     Function .  UNIV!(univ_out, data)  Memory efficient version of  UNIV() , input an  univ_out  object from  init_UNIV()  and some  data  matrix observations * variables  #  MultivariateAnomalies.init_UNIV     Function .  init_UNIV(T::Int, VAR::Int)\ninit_UNIV{tp}(data::AbstractArray{tp, 2})  initialize a  univ_out  object to be used in  UNIV!()  either with number of time steps/observations  T  and variables  VAR  or with a  data  matrix observations * variables.", 
            "title": "Univariate Approach"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#support-vector-data-description", 
            "text": "#  MultivariateAnomalies.SVDD_train     Function .  SVDD_train(K, nu)  train a one class support vecort machine model (i.e. support vector data description), given a kernel matrix K and and the highest possible percentage of outliers  nu . Returns the model object ( svdd_model ). Requires LIBSVM.  Tax, D. M. J.,   Duin, R. P. W. (1999). Support vector domain description. Pattern Recognition Letters, 20, 1191\u20131199. Sch\u00f6lkopf, B., Williamson, R. C.,   Bartlett, P. L. (2000). New Support Vector Algorithms. Neural Computation, 12, 1207\u20131245.  #  MultivariateAnomalies.SVDD_predict     Function .  SVDD_predict(svdd_model, K)  predict the outlierness of an object given the testing Kernel matrix  K  and the  svdd_model  from SVDD_train(). Requires LIBSVM.  Tax, D. M. J.,   Duin, R. P. W. (1999). Support vector domain description. Pattern Recognition Letters, 20, 1191\u20131199. Sch\u00f6lkopf, B., Williamson, R. C.,   Bartlett, P. L. (2000). New Support Vector Algorithms. Neural Computation, 12, 1207\u20131245.  #  MultivariateAnomalies.SVDD_predict!     Function .  SVDD_predict!(SVDD_out, svdd_model, K)  Memory efficient version of  SVDD_predict() . Additional input argument is the  SVDD_out  object from  init_SVDD_predict() . Compute  K with  kernel_matrix() .  SVDD_out[1]  are predicted labels,  SVDD_out[2]  decision_values. Requires LIBSVM.  #  MultivariateAnomalies.init_SVDD_predict     Function .  init_SVDD_predict(T::Int)\ninit_SVDD_predict(T::Int, Ttrain::Int)  initializes a  SVDD_out  object to be used in  SVDD_predict!() . Input is the number of time steps  T  (in prediction mode). If  T  for prediction differs from T of the training data ( Ttrain ) use  Ttrain  as additional argument.", 
            "title": "Support Vector Data Description"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#kernel-null-foley-summon-transform", 
            "text": "#  MultivariateAnomalies.KNFST_train     Function .  KNFST_train(K)  train a one class novelty KNFST model on a Kernel matrix  K  according to Paul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.  Output  (proj, targetValue)   proj     \u2013 projection vector for data points (project x via kx*proj, where kx is row vector containing kernel values of x and training data)  targetValue  \u2013 value of all training samples in the null space  #  MultivariateAnomalies.KNFST_predict     Function .  KNFST_predict(model, K)  predict the outlierness of some data (represented by the kernel matrix  K ), given some KNFST  model  from  KNFST_train(K) . Compute  K with  kernel_matrix() .  Paul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.  #  MultivariateAnomalies.KNFST_predict!     Function .  KNFST_predict!(KNFST_out, KNFST_mod, K)  predict the outlierness of some data (represented by the kernel matrix  K ), given a  KNFST_out  object ( init_KNFST() ), some KNFST model ( KNFST_mod = KNFST_train(K) ) and the testing kernel matrix K.  Paul Bodesheim and Alexander Freytag and Erik Rodner and Michael Kemmler and Joachim Denzler: \"Kernel Null Space Methods for Novelty Detection\". Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2013.  #  MultivariateAnomalies.init_KNFST     Function .  init_KNFST(T, KNFST_mod)  initialize a  KNFST_out object for the use with  KNFST_predict! , given  T , the number of observations and the model output  KNFST_train(K) .", 
            "title": "Kernel Null Foley Summon Transform"
        }, 
        {
            "location": "/man/DetectionAlgorithms/#index", 
            "text": "", 
            "title": "Index"
        }, 
        {
            "location": "/man/DistancesDensity/", 
            "text": "Distance, Kernel Matrices and k-Nearest Neighbours\n\n\nCompute distance matrices (similarity matrices) and convert them into kernel matrices or k-nearest neighbor objects.\n\n\n\n\nDistance/Similarity Matrices\n\n\nA distance matrix \nD\n consists of pairwise distances $d()$computed with some metrix (e.g. Euclidean):\n\n\n$D = d(X_{t_i}, X_{t_j})$\n\n\ni.e. the distance between vector $X$ of observation $t_i$ and $t_j$ for all observations $t_i,t_j = 1 \\ldots T$.\n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.dist_matrix\n \n \nFunction\n.\n\n\ndist_matrix{tp, N}(data::AbstractArray{tp, N}; dist::ASCIIString = \nEuclidean\n, space::Int = 0, lat::Int = 0, lon::Int = 0, Q = 0)\ndist_matrix{tp, N}(data::AbstractArray{tp, N}, training_data; dist::ASCIIString = \nEuclidean\n, space::Int = 0, lat::Int = 0, lon::Int = 0, Q = 0)\n\n\n\n\ncompute the distance matrix of \ndata\n or the distance matrix between data and training data i.e. the pairwise distances along the first dimension of data, using the last dimension as variables. \ndist\n is a distance metric, currently \nEuclidean\n(default), \nSqEuclidean\n, \nChebyshev\n, \nCityblock\n, \nJSDivergence\n, \nMahalanobis\n and \nSqMahalanobis\n are supported. The latter two need a covariance matrix \nQ\n as input argument.\n\n\nExamples\n\n\njulia\n dc = randn(10, 4,3)\njulia\n D = dist_matrix(dc, space = 2)\n\n\n\n\n#\n\n\nMultivariateAnomalies.dist_matrix!\n \n \nFunction\n.\n\n\ndist_matrix!(D_out, data, ...)\n\n\n\n\ncompute the distance matrix of \ndata\n, similar to \ndist_matrix()\n. \nD_out\n object has to be preallocated, i.e. with \ninit_dist_matrix\n.\n\n\njulia\n dc = randn(10,4, 4,3)\njulia\n D_out = init_dist_matrix(dc)\njulia\n dist_matrix!(D_out, dc, lat = 2, lon = 2)\njulia\n D_out[1]\n\n\n\n\n#\n\n\nMultivariateAnomalies.init_dist_matrix\n \n \nFunction\n.\n\n\ninit_dist_matrix(data)\ninit_dist_matrix(data, training_data)\n\n\n\n\ninitialize a \nD_out\n object for \ndist_matrix!()\n.\n\n\n\n\nk-Nearest Neighbor Objects\n\n\nk-Nearest Neighbor objects return the k nearest points and their distance out of a distance matrix \nD\n.\n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.knn_dists\n \n \nFunction\n.\n\n\nknn_dists(D, k::Int, temp_excl::Int = 5)\n\n\n\n\nreturns the k-nearest neighbors of a distance matrix \nD\n. Excludes \ntemp_excl\n (default: \ntemp_excl = 5\n) distances from the main diagonal of \nD\n to be also nearest neighbors.\n\n\njulia\n dc = randn(20, 4,3)\njulia\n D = dist_matrix(dc, space = 2)\njulia\n knn_dists_out = knn_dists(D, 3, 1)\njulia\n knn_dists_out[5] # distances\njulia\n knn_dists_out[4] # indices\n\n\n\n\n#\n\n\nMultivariateAnomalies.knn_dists!\n \n \nFunction\n.\n\n\nknn_dists!(knn_dists_out, D, temp_excl::Int = 5)\n\n\n\n\nreturns the k-nearest neighbors of a distance matrix \nD\n. Similar to \nknn_dists()\n, but uses preallocated input object \nknn_dists_out\n, initialized with \ninit_knn_dists()\n. Please note that the number of nearest neighbors \nk\n is not necessary, as it is already determined by the \nknn_dists_out\n object.\n\n\njulia\n dc = randn(20, 4,3)\njulia\n D = dist_matrix(dc, space = 2)\njulia\n knn_dists_out = init_knn_dists(dc, 3)\njulia\n knn_dists!(knn_dists_out, D)\njulia\n knn_dists_out[5] # distances\njulia\n knn_dists_out[4] # indices\n\n\n\n\n#\n\n\nMultivariateAnomalies.init_knn_dists\n \n \nFunction\n.\n\n\ninit_knn_dists(T::Int, k::Int)\ninit_knn_dists(datacube::AbstractArray, k::Int)\n\n\n\n\ninitialize a preallocated \nknn_dists_out\n object. \nk\nis the number of nerarest neighbors, \nT\n the number of time steps (i.e. size of the first dimension) or a multidimensional \ndatacube\n.\n\n\n\n\nKernel Matrices (Dissimilarities)\n\n\nA distance matrix \nD\n can be converted into a kernel matrix \nK\n, i.e. by computing pairwise dissimilarities using Gaussian kernels centered on each datapoint. \n\n\n$K= exp(-0.5 \\cdot D \\cdot \\sigma^{-2})$\n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.kernel_matrix\n \n \nFunction\n.\n\n\nkernel_matrix(D::AbstractArray, \u03c3::Float64 = 1.0[, kernel::ASCIIString = \ngauss\n, dimension::Int64 = 1])\n\n\n\n\ncompute a kernel matrix out of distance matrix \nD\n, given \n\u03c3\n. Optionally normalized by the \ndimension\n, if \nkernel = \"normalized_gauss\"\n. compute \nD\n with \ndist_matrix()\n.\n\n\njulia\n dc = randn(20, 4,3)\njulia\n D = dist_matrix(dc, space = 2)\njulia\n K = kernel_matrix(D, 2.0)\n\n\n\n\n#\n\n\nMultivariateAnomalies.kernel_matrix!\n \n \nFunction\n.\n\n\nkernel_matrix!(K, D::AbstractArray, \u03c3::Float64 = 1.0[, kernel::ASCIIString = \ngauss\n, dimension::Int64 = 1])\n\n\n\n\ncompute a kernel matrix out of distance matrix \nD\n. Similar to \nkernel_matrix()\n, but with preallocated Array K (\nK = similar(D)\n) for output.\n\n\njulia\n dc = randn(20, 4,3)\njulia\n D = dist_matrix(dc, space = 2)\njulia\n kernel_matrix!(D, D, 2.0) # overwrites distance matrix\n\n\n\n\n\n\nIndex", 
            "title": "Distances and Densities"
        }, 
        {
            "location": "/man/DistancesDensity/#distance-kernel-matrices-and-k-nearest-neighbours", 
            "text": "Compute distance matrices (similarity matrices) and convert them into kernel matrices or k-nearest neighbor objects.", 
            "title": "Distance, Kernel Matrices and k-Nearest Neighbours"
        }, 
        {
            "location": "/man/DistancesDensity/#distancesimilarity-matrices", 
            "text": "A distance matrix  D  consists of pairwise distances $d()$computed with some metrix (e.g. Euclidean):  $D = d(X_{t_i}, X_{t_j})$  i.e. the distance between vector $X$ of observation $t_i$ and $t_j$ for all observations $t_i,t_j = 1 \\ldots T$.", 
            "title": "Distance/Similarity Matrices"
        }, 
        {
            "location": "/man/DistancesDensity/#functions", 
            "text": "#  MultivariateAnomalies.dist_matrix     Function .  dist_matrix{tp, N}(data::AbstractArray{tp, N}; dist::ASCIIString =  Euclidean , space::Int = 0, lat::Int = 0, lon::Int = 0, Q = 0)\ndist_matrix{tp, N}(data::AbstractArray{tp, N}, training_data; dist::ASCIIString =  Euclidean , space::Int = 0, lat::Int = 0, lon::Int = 0, Q = 0)  compute the distance matrix of  data  or the distance matrix between data and training data i.e. the pairwise distances along the first dimension of data, using the last dimension as variables.  dist  is a distance metric, currently  Euclidean (default),  SqEuclidean ,  Chebyshev ,  Cityblock ,  JSDivergence ,  Mahalanobis  and  SqMahalanobis  are supported. The latter two need a covariance matrix  Q  as input argument.  Examples  julia  dc = randn(10, 4,3)\njulia  D = dist_matrix(dc, space = 2)  #  MultivariateAnomalies.dist_matrix!     Function .  dist_matrix!(D_out, data, ...)  compute the distance matrix of  data , similar to  dist_matrix() .  D_out  object has to be preallocated, i.e. with  init_dist_matrix .  julia  dc = randn(10,4, 4,3)\njulia  D_out = init_dist_matrix(dc)\njulia  dist_matrix!(D_out, dc, lat = 2, lon = 2)\njulia  D_out[1]  #  MultivariateAnomalies.init_dist_matrix     Function .  init_dist_matrix(data)\ninit_dist_matrix(data, training_data)  initialize a  D_out  object for  dist_matrix!() .", 
            "title": "Functions"
        }, 
        {
            "location": "/man/DistancesDensity/#k-nearest-neighbor-objects", 
            "text": "k-Nearest Neighbor objects return the k nearest points and their distance out of a distance matrix  D .", 
            "title": "k-Nearest Neighbor Objects"
        }, 
        {
            "location": "/man/DistancesDensity/#functions_1", 
            "text": "#  MultivariateAnomalies.knn_dists     Function .  knn_dists(D, k::Int, temp_excl::Int = 5)  returns the k-nearest neighbors of a distance matrix  D . Excludes  temp_excl  (default:  temp_excl = 5 ) distances from the main diagonal of  D  to be also nearest neighbors.  julia  dc = randn(20, 4,3)\njulia  D = dist_matrix(dc, space = 2)\njulia  knn_dists_out = knn_dists(D, 3, 1)\njulia  knn_dists_out[5] # distances\njulia  knn_dists_out[4] # indices  #  MultivariateAnomalies.knn_dists!     Function .  knn_dists!(knn_dists_out, D, temp_excl::Int = 5)  returns the k-nearest neighbors of a distance matrix  D . Similar to  knn_dists() , but uses preallocated input object  knn_dists_out , initialized with  init_knn_dists() . Please note that the number of nearest neighbors  k  is not necessary, as it is already determined by the  knn_dists_out  object.  julia  dc = randn(20, 4,3)\njulia  D = dist_matrix(dc, space = 2)\njulia  knn_dists_out = init_knn_dists(dc, 3)\njulia  knn_dists!(knn_dists_out, D)\njulia  knn_dists_out[5] # distances\njulia  knn_dists_out[4] # indices  #  MultivariateAnomalies.init_knn_dists     Function .  init_knn_dists(T::Int, k::Int)\ninit_knn_dists(datacube::AbstractArray, k::Int)  initialize a preallocated  knn_dists_out  object.  k is the number of nerarest neighbors,  T  the number of time steps (i.e. size of the first dimension) or a multidimensional  datacube .", 
            "title": "Functions"
        }, 
        {
            "location": "/man/DistancesDensity/#kernel-matrices-dissimilarities", 
            "text": "A distance matrix  D  can be converted into a kernel matrix  K , i.e. by computing pairwise dissimilarities using Gaussian kernels centered on each datapoint.   $K= exp(-0.5 \\cdot D \\cdot \\sigma^{-2})$", 
            "title": "Kernel Matrices (Dissimilarities)"
        }, 
        {
            "location": "/man/DistancesDensity/#functions_2", 
            "text": "#  MultivariateAnomalies.kernel_matrix     Function .  kernel_matrix(D::AbstractArray, \u03c3::Float64 = 1.0[, kernel::ASCIIString =  gauss , dimension::Int64 = 1])  compute a kernel matrix out of distance matrix  D , given  \u03c3 . Optionally normalized by the  dimension , if  kernel = \"normalized_gauss\" . compute  D  with  dist_matrix() .  julia  dc = randn(20, 4,3)\njulia  D = dist_matrix(dc, space = 2)\njulia  K = kernel_matrix(D, 2.0)  #  MultivariateAnomalies.kernel_matrix!     Function .  kernel_matrix!(K, D::AbstractArray, \u03c3::Float64 = 1.0[, kernel::ASCIIString =  gauss , dimension::Int64 = 1])  compute a kernel matrix out of distance matrix  D . Similar to  kernel_matrix() , but with preallocated Array K ( K = similar(D) ) for output.  julia  dc = randn(20, 4,3)\njulia  D = dist_matrix(dc, space = 2)\njulia  kernel_matrix!(D, D, 2.0) # overwrites distance matrix", 
            "title": "Functions"
        }, 
        {
            "location": "/man/DistancesDensity/#index", 
            "text": "", 
            "title": "Index"
        }, 
        {
            "location": "/man/Preprocessing/", 
            "text": "Feature Extraction Techniques\n\n\nExtract the relevant inforamtion out of your data and use them as input feature for the anomaly detection algorithms.\n\n\n\n\nDimensionality Reduction\n\n\nCurrently two dimenionality reduction techniques are implemented from \nMultivariateStats.jl\n:\n\n\n\n\nPrincipal Component Analysis (PCA)\n\n\nIndependent Component Analysis (ICA)\n\n\n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.globalPCA\n \n \nFunction\n.\n\n\nglobalPCA{tp, N}(datacube::Array{tp, N}, expl_var::Float64 = 0.95)\n\n\n\n\nreturn an orthogonal subset of the variables, i.e. the last dimension of the datacube. A Principal Component Analysis is performed on the entire datacube, explaining at least \nexpl_var\n of the variance.\n\n\n#\n\n\nMultivariateAnomalies.globalICA\n \n \nFunction\n.\n\n\nglobalICA(datacube::Array{tp, 4}, mode = \nexpl_var\n; expl_var::Float64 = 0.95, num_comp::Int = 3)\n\n\n\n\nperform an Independent Component Analysis on the entire 4-dimensional datacube either by (\nmode = \"num_comp\"\n) returning num_comp number of independent components or (\nmode = \"expl_var\"\n) returning the number of components which is necessary to explain expl_var of the variance, when doing a Prinicpal Component Analysis before.\n\n\n\n\nSeasonality\n\n\nWhen dealing with time series, i.e. the observations are time steps, it might be important to remove or get robust estimates of the mean seasonal cycles. This is implemended by\n\n\n\n\nsubtracting the median seasonal cycle (sMSC) and\n\n\ngetting the median seasonal cycle (get_MedianCycles)\n\n\n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.sMSC\n \n \nFunction\n.\n\n\nsMSC(datacube, cycle_length)\n\n\n\n\nsubtract the median seasonal cycle from the datacube given the length of year \ncycle_length\n.\n\n\nExamples\n\n\njulia\n dc = hcat(rand(193) + 2* sin(0:pi/24:8*pi), rand(193) + 2* sin(0:pi/24:8*pi))\njulia\n sMSC_dc = sMSC(dc, 48)\n\n\n\n\n#\n\n\nMultivariateAnomalies.get_MedianCycles\n \n \nFunction\n.\n\n\nget_MedianCycles(datacube, cycle_length::Int = 46)\n\n\n\n\nreturns the median annual cycle of a datacube, given the length of the annual cycle (presetting: \ncycle_length = 46\n). The datacube can be 2, 3, 4-dimensional, time is stored along the first dimension.\n\n\nExamples\n\n\njulia\n dc = hcat(rand(193) + 2* sin(0:pi/24:8*pi), rand(193) + 2* sin(0:pi/24:8*pi))\njulia\n cycles = get_MedianCycles(dc, 48)\n\n\n\n\n#\n\n\nMultivariateAnomalies.get_MedianCycle\n \n \nFunction\n.\n\n\nget_MedianCycle(dat::Array{tp,1}, cycle_length::Int = 46)\n\n\n\n\nreturns the median annual cycle of a one dimensional data array, given the length of the annual cycle (presetting: \ncycle_length = 46\n). Can deal with some NaN values.\n\n\nExamples\n\n\njulia\n dat = rand(193) + 2* sin(0:pi/24:8*pi)\njulia\n dat[100] = NaN\njulia\n cycles = get_MedianCycle(dat, 48)\n\n\n\n\n#\n\n\nMultivariateAnomalies.get_MedianCycle!\n \n \nFunction\n.\n\n\nget_MedianCycle!(init_MC, dat::Array{tp,1})\n\n\n\n\nMemory efficient version of \nget_MedianCycle()\n, returning the median cycle in \ninit_MC[3]\n. The \ninit_MC\n object should be created with \ninit_MedianCycle\n. Can deal with some NaN values.\n\n\nExamples\n\n\njulia\n dat = rand(193) + 2* sin(0:pi/24:8*pi)\njulia\n dat[100] = NaN\njulia\n init_MC = init_MedianCycle(dat, 48)\njulia\n get_MedianCycle!(init_MC, dat)\njulia\n init_MC[3]\n\n\n\n\n#\n\n\nMultivariateAnomalies.init_MedianCycle\n \n \nFunction\n.\n\n\ninit_MedianCycle(dat::Array{tp}, cycle_length::Int = 46)\ninit_MedianCycle(temporal_length::Int[, cycle_length::Int = 46])\n\n\n\n\ninitialises an init_MC object to be used as input for \nget_MedianCycle!()\n. Input is either some sample data or the temporal lenght of the expected input vector and the length of the annual cycle (presetting: \ncycle_length = 46\n)\n\n\n\n\nExponential Weighted Moving Average\n\n\nOne option to reduce the noise level in the data and detect more 'significant' anomalies is computing an exponential weighted moving average (EWMA)\n\n\n\n\nFunction\n\n\n#\n\n\nMultivariateAnomalies.EWMA\n \n \nFunction\n.\n\n\nEWMA(dat,  \u03bb)\n\n\n\n\nCompute the exponential weighted moving average (EWMA) with the weighting parameter \n\u03bb\n between 0 (full weighting) and 1 (no weighting) along the first dimension of \ndat\n. Supports N-dimensional Arrays.\n\n\nLowry, C. A., \n Woodall, W. H. (1992). A Multivariate Exponentially Weighted Moving Average Control Chart. Technometrics, 34, 46\u201353.\n\n\njulia\n dc = rand(100,3,2)\njulia\n ewma_dc = EWMA(dc, 0.1)\n\n\n\n\n#\n\n\nMultivariateAnomalies.EWMA!\n \n \nFunction\n.\n\n\nEWMA!(Z, dat,  \u03bb)\n\n\n\n\nuse a preallocated output Z. \nZ = similar(dat)\n or \ndat = dat\n for overwriting itself.\n\n\nExamples\n\n\njulia\n dc = rand(100,3,2)\njulia\n EWMA!(dc, dc, 0.1)\n\n\n\n\n\n\nTime Delay Embedding\n\n\nIncrease the feature space (Variabales) with lagged observations. \n\n\n\n\nFunction\n\n\n#\n\n\nMultivariateAnomalies.TDE\n \n \nFunction\n.\n\n\nTDE{tp}(datacube::Array{tp, 4}, \u0394T::Integer, DIM::Int = 3)\nTDE{tp}(datacube::Array{tp, 3}, \u0394T::Integer, DIM::Int = 3)\n\n\n\n\nreturns an embedded datacube by concatenating lagged versions of the 2-, 3- or 4-dimensional datacube with \n\u0394T\n time steps in the past up to dimension \nDIM\n (presetting: \nDIM = 3\n)\n\n\njulia\n dc = randn(50,3)\njulia\n TDE(dc, 3, 2)\n\n\n\n\n\n\nMoving Window Features\n\n\ninclude the variance (mw_VAR) and correlations (mw_COR) in a moving window along the first dimension of the data.\n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.mw_VAR\n \n \nFunction\n.\n\n\nmw_VAR{tp,N}(datacube::Array{tp,N}, windowsize::Int = 10)\n\n\n\n\ncompute the variance in a moving window along the first dimension of the datacube (presetting: \nwindowsize = 10\n). Accepts N dimensional datacubes.\n\n\njulia\n dc = randn(50,3,3,3)\njulia\n mw_VAR(dc, 15)\n\n\n\n\n#\n\n\nMultivariateAnomalies.mw_COR\n \n \nFunction\n.\n\n\nmw_COR{tp}(datacube::Array{tp, 4}, windowsize::Int = 10)\n\n\n\n\ncompute the correlation in a moving window along the first dimension of the datacube (presetting: \nwindowsize = 10\n). Accepts 4-dimensional datacubes.\n\n\n\n\nIndex", 
            "title": "Preprocessing"
        }, 
        {
            "location": "/man/Preprocessing/#feature-extraction-techniques", 
            "text": "Extract the relevant inforamtion out of your data and use them as input feature for the anomaly detection algorithms.", 
            "title": "Feature Extraction Techniques"
        }, 
        {
            "location": "/man/Preprocessing/#dimensionality-reduction", 
            "text": "Currently two dimenionality reduction techniques are implemented from  MultivariateStats.jl :   Principal Component Analysis (PCA)  Independent Component Analysis (ICA)", 
            "title": "Dimensionality Reduction"
        }, 
        {
            "location": "/man/Preprocessing/#functions", 
            "text": "#  MultivariateAnomalies.globalPCA     Function .  globalPCA{tp, N}(datacube::Array{tp, N}, expl_var::Float64 = 0.95)  return an orthogonal subset of the variables, i.e. the last dimension of the datacube. A Principal Component Analysis is performed on the entire datacube, explaining at least  expl_var  of the variance.  #  MultivariateAnomalies.globalICA     Function .  globalICA(datacube::Array{tp, 4}, mode =  expl_var ; expl_var::Float64 = 0.95, num_comp::Int = 3)  perform an Independent Component Analysis on the entire 4-dimensional datacube either by ( mode = \"num_comp\" ) returning num_comp number of independent components or ( mode = \"expl_var\" ) returning the number of components which is necessary to explain expl_var of the variance, when doing a Prinicpal Component Analysis before.", 
            "title": "Functions"
        }, 
        {
            "location": "/man/Preprocessing/#seasonality", 
            "text": "When dealing with time series, i.e. the observations are time steps, it might be important to remove or get robust estimates of the mean seasonal cycles. This is implemended by   subtracting the median seasonal cycle (sMSC) and  getting the median seasonal cycle (get_MedianCycles)", 
            "title": "Seasonality"
        }, 
        {
            "location": "/man/Preprocessing/#functions_1", 
            "text": "#  MultivariateAnomalies.sMSC     Function .  sMSC(datacube, cycle_length)  subtract the median seasonal cycle from the datacube given the length of year  cycle_length .  Examples  julia  dc = hcat(rand(193) + 2* sin(0:pi/24:8*pi), rand(193) + 2* sin(0:pi/24:8*pi))\njulia  sMSC_dc = sMSC(dc, 48)  #  MultivariateAnomalies.get_MedianCycles     Function .  get_MedianCycles(datacube, cycle_length::Int = 46)  returns the median annual cycle of a datacube, given the length of the annual cycle (presetting:  cycle_length = 46 ). The datacube can be 2, 3, 4-dimensional, time is stored along the first dimension.  Examples  julia  dc = hcat(rand(193) + 2* sin(0:pi/24:8*pi), rand(193) + 2* sin(0:pi/24:8*pi))\njulia  cycles = get_MedianCycles(dc, 48)  #  MultivariateAnomalies.get_MedianCycle     Function .  get_MedianCycle(dat::Array{tp,1}, cycle_length::Int = 46)  returns the median annual cycle of a one dimensional data array, given the length of the annual cycle (presetting:  cycle_length = 46 ). Can deal with some NaN values.  Examples  julia  dat = rand(193) + 2* sin(0:pi/24:8*pi)\njulia  dat[100] = NaN\njulia  cycles = get_MedianCycle(dat, 48)  #  MultivariateAnomalies.get_MedianCycle!     Function .  get_MedianCycle!(init_MC, dat::Array{tp,1})  Memory efficient version of  get_MedianCycle() , returning the median cycle in  init_MC[3] . The  init_MC  object should be created with  init_MedianCycle . Can deal with some NaN values.  Examples  julia  dat = rand(193) + 2* sin(0:pi/24:8*pi)\njulia  dat[100] = NaN\njulia  init_MC = init_MedianCycle(dat, 48)\njulia  get_MedianCycle!(init_MC, dat)\njulia  init_MC[3]  #  MultivariateAnomalies.init_MedianCycle     Function .  init_MedianCycle(dat::Array{tp}, cycle_length::Int = 46)\ninit_MedianCycle(temporal_length::Int[, cycle_length::Int = 46])  initialises an init_MC object to be used as input for  get_MedianCycle!() . Input is either some sample data or the temporal lenght of the expected input vector and the length of the annual cycle (presetting:  cycle_length = 46 )", 
            "title": "Functions"
        }, 
        {
            "location": "/man/Preprocessing/#exponential-weighted-moving-average", 
            "text": "One option to reduce the noise level in the data and detect more 'significant' anomalies is computing an exponential weighted moving average (EWMA)", 
            "title": "Exponential Weighted Moving Average"
        }, 
        {
            "location": "/man/Preprocessing/#function", 
            "text": "#  MultivariateAnomalies.EWMA     Function .  EWMA(dat,  \u03bb)  Compute the exponential weighted moving average (EWMA) with the weighting parameter  \u03bb  between 0 (full weighting) and 1 (no weighting) along the first dimension of  dat . Supports N-dimensional Arrays.  Lowry, C. A.,   Woodall, W. H. (1992). A Multivariate Exponentially Weighted Moving Average Control Chart. Technometrics, 34, 46\u201353.  julia  dc = rand(100,3,2)\njulia  ewma_dc = EWMA(dc, 0.1)  #  MultivariateAnomalies.EWMA!     Function .  EWMA!(Z, dat,  \u03bb)  use a preallocated output Z.  Z = similar(dat)  or  dat = dat  for overwriting itself.  Examples  julia  dc = rand(100,3,2)\njulia  EWMA!(dc, dc, 0.1)", 
            "title": "Function"
        }, 
        {
            "location": "/man/Preprocessing/#time-delay-embedding", 
            "text": "Increase the feature space (Variabales) with lagged observations.", 
            "title": "Time Delay Embedding"
        }, 
        {
            "location": "/man/Preprocessing/#function_1", 
            "text": "#  MultivariateAnomalies.TDE     Function .  TDE{tp}(datacube::Array{tp, 4}, \u0394T::Integer, DIM::Int = 3)\nTDE{tp}(datacube::Array{tp, 3}, \u0394T::Integer, DIM::Int = 3)  returns an embedded datacube by concatenating lagged versions of the 2-, 3- or 4-dimensional datacube with  \u0394T  time steps in the past up to dimension  DIM  (presetting:  DIM = 3 )  julia  dc = randn(50,3)\njulia  TDE(dc, 3, 2)", 
            "title": "Function"
        }, 
        {
            "location": "/man/Preprocessing/#moving-window-features", 
            "text": "include the variance (mw_VAR) and correlations (mw_COR) in a moving window along the first dimension of the data.", 
            "title": "Moving Window Features"
        }, 
        {
            "location": "/man/Preprocessing/#functions_2", 
            "text": "#  MultivariateAnomalies.mw_VAR     Function .  mw_VAR{tp,N}(datacube::Array{tp,N}, windowsize::Int = 10)  compute the variance in a moving window along the first dimension of the datacube (presetting:  windowsize = 10 ). Accepts N dimensional datacubes.  julia  dc = randn(50,3,3,3)\njulia  mw_VAR(dc, 15)  #  MultivariateAnomalies.mw_COR     Function .  mw_COR{tp}(datacube::Array{tp, 4}, windowsize::Int = 10)  compute the correlation in a moving window along the first dimension of the datacube (presetting:  windowsize = 10 ). Accepts 4-dimensional datacubes.", 
            "title": "Functions"
        }, 
        {
            "location": "/man/Preprocessing/#index", 
            "text": "", 
            "title": "Index"
        }, 
        {
            "location": "/man/Postprocessing/", 
            "text": "Scores\n\n\nPostprocess your anomaly scores by making different algorithms comparable and computing their ensemble.\n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.get_quantile_scores\n \n \nFunction\n.\n\n\nget_quantile_scores(scores, quantiles = 0.0:0.01:1.0)\n\n\n\n\nreturn the quantiles of the given N dimensional anomaly \nscores\n cube. \nquantiles\n (default: \nquantiles = 0.0:0.01:1.0\n) is a Float range of quantiles. Any score being greater or equal \nquantiles[i]\n and beeing smaller than \nquantiles[i+1]\n is assigned to the respective quantile \nquantiles[i]\n.\n\n\nExamples\n\n\njulia\n scores1 = rand(10, 2)\njulia\n quantile_scores1 = get_quantile_scores(scores1)\n\n\n\n\n#\n\n\nMultivariateAnomalies.get_quantile_scores!\n \n \nFunction\n.\n\n\nget_quantile_scores!{tp,N}(quantile_scores::AbstractArray{Float64, N}, scores::AbstractArray{tp,N}, quantiles::FloatRange{Float64} = 0.0:0.01:1.0)\n\n\n\n\nreturn the quantiles of the given N dimensional \nscores\n array into a preallocated \nquantile_scores\n array, see \nget_quantile_scores()\n.\n\n\n#\n\n\nMultivariateAnomalies.compute_ensemble\n \n \nFunction\n.\n\n\ncompute_ensemble(m1_scores, m2_scores[, m3_scores, m4_scores]; ensemble = \nmean\n)\n\n\n\n\ncompute the mean (\nensemble = \"mean\"\n), minimum (\nensemble = \"min\"\n), maximum (\nensemble = \"max\"\n) or median (\nensemble = \"median\"\n) of the given anomaly scores. Supports between 2 and 4 scores input arrays (\nm1_scores, ..., m4_scores\n). The scores of the different anomaly detection algorithms should be somehow comparable, e.g., by using \nget_quantile_scores()\n before.\n\n\nExamples\n\n\njulia\n scores1 = rand(10, 2)\njulia\n scores2 = rand(10, 2)\njulia\n quantile_scores1 = get_quantile_scores(scores1)\njulia\n quantile_scores2 = get_quantile_scores(scores2)\njulia\n compute_ensemble(quantile_scores1, quantile_scores2, ensemble = \nmax\n)\n\n\n\n\n\n\nIndex", 
            "title": "Postprocessing"
        }, 
        {
            "location": "/man/Postprocessing/#scores", 
            "text": "Postprocess your anomaly scores by making different algorithms comparable and computing their ensemble.", 
            "title": "Scores"
        }, 
        {
            "location": "/man/Postprocessing/#functions", 
            "text": "#  MultivariateAnomalies.get_quantile_scores     Function .  get_quantile_scores(scores, quantiles = 0.0:0.01:1.0)  return the quantiles of the given N dimensional anomaly  scores  cube.  quantiles  (default:  quantiles = 0.0:0.01:1.0 ) is a Float range of quantiles. Any score being greater or equal  quantiles[i]  and beeing smaller than  quantiles[i+1]  is assigned to the respective quantile  quantiles[i] .  Examples  julia  scores1 = rand(10, 2)\njulia  quantile_scores1 = get_quantile_scores(scores1)  #  MultivariateAnomalies.get_quantile_scores!     Function .  get_quantile_scores!{tp,N}(quantile_scores::AbstractArray{Float64, N}, scores::AbstractArray{tp,N}, quantiles::FloatRange{Float64} = 0.0:0.01:1.0)  return the quantiles of the given N dimensional  scores  array into a preallocated  quantile_scores  array, see  get_quantile_scores() .  #  MultivariateAnomalies.compute_ensemble     Function .  compute_ensemble(m1_scores, m2_scores[, m3_scores, m4_scores]; ensemble =  mean )  compute the mean ( ensemble = \"mean\" ), minimum ( ensemble = \"min\" ), maximum ( ensemble = \"max\" ) or median ( ensemble = \"median\" ) of the given anomaly scores. Supports between 2 and 4 scores input arrays ( m1_scores, ..., m4_scores ). The scores of the different anomaly detection algorithms should be somehow comparable, e.g., by using  get_quantile_scores()  before.  Examples  julia  scores1 = rand(10, 2)\njulia  scores2 = rand(10, 2)\njulia  quantile_scores1 = get_quantile_scores(scores1)\njulia  quantile_scores2 = get_quantile_scores(scores2)\njulia  compute_ensemble(quantile_scores1, quantile_scores2, ensemble =  max )", 
            "title": "Functions"
        }, 
        {
            "location": "/man/Postprocessing/#index", 
            "text": "", 
            "title": "Index"
        }, 
        {
            "location": "/man/AUC/", 
            "text": "Area Under the Curve\n\n\nCompute true positive rates, false positive rates and the area under the curve to evaulate the algorihtms performance. Efficient implementation according to\n\n\nFawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861\u2013874. \nLink\n\n\n\n\nFunctions\n\n\n#\n\n\nMultivariateAnomalies.auc\n \n \nFunction\n.\n\n\nauc(scores, events, increasing = true)\n\n\n\n\ncompute the Area Under the receiver operator Curve (AUC), given some output \nscores\n array and some ground truth (\nevents\n). By default, it is assumed, that the \nscores\n are ordered increasingly (\nincreasing = true\n), i.e. high scores represent events.\n\n\nExamples\n\n\njulia\n scores = rand(10, 2)\njulia\n events = rand(0:1, 10, 2)\njulia\n auc(scores, events)\njulia\n auc(scores, boolevents(events))\n\n\n\n\n#\n\n\nMultivariateAnomalies.auc_fpr_tpr\n \n \nFunction\n.\n\n\nauc_fpr_tpr(scores, events, quant = 0.9, increasing = true)\n\n\n\n\nSimilar like \nauc()\n, but return additionally the true positive and false positive rate at a given quantile (default: \nquant = 0.9\n).\n\n\nExamples\n\n\njulia\n scores = rand(10, 2)\njulia\n events = rand(0:1, 10, 2)\njulia\n auc_fpr_tpr(scores, events, 0.8)\n\n\n\n\n#\n\n\nMultivariateAnomalies.boolevents\n \n \nFunction\n.\n\n\nboolevents(events)\n\n\n\n\nconvert an \nevents\n array into a boolean array.\n\n\nIndex", 
            "title": "AUC"
        }, 
        {
            "location": "/man/AUC/#area-under-the-curve", 
            "text": "Compute true positive rates, false positive rates and the area under the curve to evaulate the algorihtms performance. Efficient implementation according to  Fawcett, T. (2006). An introduction to ROC analysis. Pattern Recognition Letters, 27(8), 861\u2013874.  Link", 
            "title": "Area Under the Curve"
        }, 
        {
            "location": "/man/AUC/#functions", 
            "text": "#  MultivariateAnomalies.auc     Function .  auc(scores, events, increasing = true)  compute the Area Under the receiver operator Curve (AUC), given some output  scores  array and some ground truth ( events ). By default, it is assumed, that the  scores  are ordered increasingly ( increasing = true ), i.e. high scores represent events.  Examples  julia  scores = rand(10, 2)\njulia  events = rand(0:1, 10, 2)\njulia  auc(scores, events)\njulia  auc(scores, boolevents(events))  #  MultivariateAnomalies.auc_fpr_tpr     Function .  auc_fpr_tpr(scores, events, quant = 0.9, increasing = true)  Similar like  auc() , but return additionally the true positive and false positive rate at a given quantile (default:  quant = 0.9 ).  Examples  julia  scores = rand(10, 2)\njulia  events = rand(0:1, 10, 2)\njulia  auc_fpr_tpr(scores, events, 0.8)  #  MultivariateAnomalies.boolevents     Function .  boolevents(events)  convert an  events  array into a boolean array.  Index", 
            "title": "Functions"
        }
    ]
}